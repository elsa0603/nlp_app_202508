import io
import re
import json
import textwrap
import streamlit as st

from typing import List, Tuple

# ---------- Optional imports for file parsing ----------
try:
    import docx2txt
except Exception:
    docx2txt = None

try:
    import PyPDF2
except Exception:
    PyPDF2 = None

# URL fetching
try:
    import requests
    from bs4 import BeautifulSoup
    from readability import Document
except Exception:
    requests = None
    BeautifulSoup = None
    Document = None

from transformers import pipeline

st.set_page_config(page_title="NLP Toolkit (HF Pipelines) + URL", layout="wide")

st.title("ğŸ¤— NLP Toolkit â€” Transformers Pipelines")
st.caption("æƒ…ç·’åˆ†æã€å•ç­”ã€æ‘˜è¦ã€å‘½åå¯¦é«”è¾¨è­˜ã€é›¶æ¨£æœ¬åˆ†é¡ï½œæ”¯æ´ä¸Šå‚³ .txt/.pdf/.docxã€è²¼ä¸Šæ–‡å­—ã€èˆ‡æŠ“å–ç¶²é å…§å®¹ï¼ˆURLï¼‰")

# --------------------- Helpers ---------------------
def read_text_from_file(uploaded_file) -> str:
    if uploaded_file is None:
        return ""
    name = uploaded_file.name.lower()
    if name.endswith(".txt"):
        return uploaded_file.getvalue().decode("utf-8", errors="ignore")
    if name.endswith(".pdf"):
        if PyPDF2 is None:
            st.error("æœªå®‰è£ PyPDF2ï¼Œè«‹åœ¨ requirements.txt åŠ å…¥ `PyPDF2`ã€‚")
            return ""
        try:
            reader = PyPDF2.PdfReader(uploaded_file)
            pages = []
            for page in reader.pages:
                try:
                    pages.append(page.extract_text() or "")
                except Exception:
                    pages.append("")
            return "\\n".join(pages)
        except Exception as e:
            st.error(f"PDF è§£æå¤±æ•—ï¼š{e}")
            return ""
    if name.endswith(".docx"):
        if docx2txt is None:
            st.error("æœªå®‰è£ docx2txtï¼Œè«‹åœ¨ requirements.txt åŠ å…¥ `docx2txt`ã€‚")
            return ""
        try:
            content = uploaded_file.getvalue()
            with open("tmp_docx.docx", "wb") as f:
                f.write(content)
            text = docx2txt.process("tmp_docx.docx") or ""
            return text
        except Exception as e:
            st.error(f"DOCX è§£æå¤±æ•—ï¼š{e}")
            return ""
    st.warning("ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼Œè«‹ä¸Šå‚³ .txt / .pdf / .docx")
    return ""


def chunk_text(text: str, max_chars: int = 1500, overlap: int = 100) -> List[str]:
    text = re.sub(r"\\s+", " ", text).strip()
    if not text:
        return []
    chunks = []
    start = 0
    n = len(text)
    while start < n:
        end = min(start + max_chars, n)
        # try to cut on punctuation
        if end < n:
            cut = text.rfind(".", start, end)
            cut = max(cut, text.rfind("ã€‚", start, end))
            cut = max(cut, text.rfind("ï¼", start, end))
            cut = max(cut, text.rfind("ï¼Ÿ", start, end))
            if cut != -1 and cut > start + max_chars * 0.6:
                end = cut + 1
        chunks.append(text[start:end].strip())
        start = max(end - overlap, end)
    return [c for c in chunks if c]


@st.cache_resource(show_spinner=False)
def get_pipeline(task: str, model_id: str = None, tokenizer_id: str = None, device_map: str = "auto"):
    kwargs = {}
    if model_id:
        kwargs["model"] = model_id
    if tokenizer_id:
        kwargs["tokenizer"] = tokenizer_id
    try:
        return pipeline(task, device_map=device_map, **kwargs)
    except TypeError:
        return pipeline(task, **kwargs)


def show_json_download(data, label="ä¸‹è¼‰JSON", filename="result.json"):
    j = json.dumps(data, ensure_ascii=False, indent=2)
    st.download_button(label, j, file_name=filename, mime="application/json")


def is_url(s: str) -> bool:
    return bool(re.match(r"^https?://", s.strip(), re.I))


def fetch_url_to_text(url: str, timeout: int = 15) -> Tuple[str, str]:
    "\"\"\"Fetch a URL and return (title, text). Requires requests, BeautifulSoup, readability-lxml.\"\"\""
    if requests is None or BeautifulSoup is None or Document is None:
        raise RuntimeError("ç¼ºå°‘ requests/bs4/readability-lxmlï¼Œè«‹åœ¨ requirements.txt å®‰è£ã€‚")

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36"
    }
    r = requests.get(url, headers=headers, timeout=timeout)
    r.raise_for_status()

    # Use readability to isolate main content
    doc = Document(r.text)
    title = doc.short_title() or ""
    content_html = doc.summary()
    soup = BeautifulSoup(content_html, "lxml")
    # Remove scripts/styles
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()
    text = soup.get_text("\\n", strip=True)

    # Fallback if text too short
    if len(text) < 200:
        soup2 = BeautifulSoup(r.text, "lxml")
        for tag in soup2(["script", "style", "noscript"]):
            tag.decompose()
        text2 = soup2.get_text("\\n", strip=True)
        if len(text2) > len(text):
            text = text2
    return title, text


# --------------------- Sidebar ---------------------
st.sidebar.header("âš™ï¸ è¨­å®š")

task = st.sidebar.selectbox(
    "é¸æ“‡ä»»å‹™ (Task)",
    ["Sentiment Analysis", "Question Answering", "Summarization", "Named Entity Recognition", "Zero-shot Classification"],
    index=0
)

lang = st.sidebar.radio("èªè¨€/æ¨¡å‹é¸æ“‡", ["ä¸­æ–‡å„ªå…ˆ", "è‹±æ–‡å„ªå…ˆ", "å¤šèª/é€šç”¨"], index=0)

# Preselect model IDs
model_map = {
    "Sentiment Analysis": {
        "ä¸­æ–‡å„ªå…ˆ": ("uer/roberta-base-finetuned-jd-binary-chinese", None),
        "è‹±æ–‡å„ªå…ˆ": (None, None),
        "å¤šèª/é€šç”¨": ("cardiffnlp/twitter-xlm-roberta-base-sentiment", None),
    },
    "Question Answering": {
        "ä¸­æ–‡å„ªå…ˆ": ("uer/roberta-base-chinese-extractive-qa", None),
        "è‹±æ–‡å„ªå…ˆ": (None, None),
        "å¤šèª/é€šç”¨": ("deepset/xlm-roberta-base-squad2", None),
    },
    "Summarization": {
        "ä¸­æ–‡å„ªå…ˆ": ("csebuetnlp/mT5_multilingual_XLSum", None),
        "è‹±æ–‡å„ªå…ˆ": ("facebook/bart-large-cnn", None),
        "å¤šèª/é€šç”¨": ("csebuetnlp/mT5_multilingual_XLSum", None),
    },
    "Named Entity Recognition": {
        "ä¸­æ–‡å„ªå…ˆ": ("ckiplab/bert-base-chinese-ner", "ckiplab/bert-base-chinese-ner"),
        "è‹±æ–‡å„ªå…ˆ": ("dslim/bert-base-NER", None),
        "å¤šèª/é€šç”¨": ("Davlan/xlm-roberta-base-ner-hrl", None),
    },
    "Zero-shot Classification": {
        "ä¸­æ–‡å„ªå…ˆ": ("joeddav/xlm-roberta-large-xnli", None),
        "è‹±æ–‡å„ªå…ˆ": ("facebook/bart-large-mnli", None),
        "å¤šèª/é€šç”¨": ("joeddav/xlm-roberta-large-xnli", None),
    },
}

model_id, tokenizer_id = model_map[task][lang]
st.sidebar.write("**Model:**", model_id or "(default)")
if tokenizer_id:
    st.sidebar.write("**Tokenizer:**", tokenizer_id)

max_new_tokens = st.sidebar.slider("ç”¢ç”Ÿ/æ‘˜è¦é•·åº¦ï¼ˆæœ€å¤§å­—å…ƒä¼°è¨ˆï¼‰", 32, 2048, 256) if task == "Summarization" else None

# --------------------- Inputs ---------------------
col_left, col_right = st.columns([1, 1])

with col_left:
    uploaded = st.file_uploader("ğŸ“ ä¸Šå‚³æª”æ¡ˆï¼ˆ.txt / .pdf / .docxï¼‰", type=["txt", "pdf", "docx"])
    pasted = st.text_area("æˆ–è²¼ä¸Šæ–‡å­— / context", height=160, placeholder="åœ¨æ­¤è²¼ä¸Šé•·æ–‡ã€æ–‡ç« æˆ–å•ç­” contextâ€¦")
    url = st.text_input("æˆ–è¼¸å…¥ç¶²å€(URL) æŠ“å–å…§å®¹")
    fetch_btn = st.button("ğŸ” æŠ“å–ç¶²å€")
    if fetch_btn:
        if not url or not is_url(url):
            st.warning("è«‹è¼¸å…¥æœ‰æ•ˆçš„ http/https ç¶²å€")
        else:
            try:
                title, txt = fetch_url_to_text(url)
                st.session_state["fetched_title"] = title
                st.session_state["fetched_text"] = txt
                st.success(f"å·²æŠ“å–ï¼š{title or url}")
            except Exception as e:
                st.error(f"æŠ“å–å¤±æ•—ï¼š{e}")

with col_right:
    if task == "Question Answering":
        question = st.text_input("å•é¡Œ (Question)", value="å°ç£çš„é¦–éƒ½æ˜¯å“ªè£¡ï¼Ÿ")
    elif task == "Zero-shot Classification":
        labels_text = st.text_input("å€™é¸æ¨™ç±¤ï¼ˆä»¥é€—è™Ÿåˆ†éš”ï¼‰", value="æ­£é¢, è² é¢, ä¸­ç«‹, åƒ¹æ ¼, æœå‹™, ç’°å¢ƒ")
        multi_label = st.checkbox("å¤šæ¨™ç±¤ (ä¸€æ®µæ–‡å­—å¯èƒ½å±¬æ–¼å¤šå€‹æ¨™ç±¤)", value=True)
    else:
        st.empty()

# Preview fetched content
if "fetched_text" in st.session_state and st.session_state.get("fetched_text"):
    with st.expander("å·²æŠ“å–çš„å…§å®¹é è¦½", expanded=False):
        st.markdown(f"**æ¨™é¡Œï¼š** {st.session_state.get('fetched_title','(ç„¡)')}")
        st.text_area("å…§å®¹", st.session_state["fetched_text"][:4000], height=200)

# Combine sources
source_text = ""
if uploaded is not None:
    source_text = read_text_from_file(uploaded)
if pasted.strip():
    joiner = "\\n\\n" if source_text else ""
    source_text = (source_text + joiner + pasted.strip())
if st.session_state.get("fetched_text"):
    joiner = "\\n\\n" if source_text else ""
    source_text = (source_text + joiner + st.session_state["fetched_text"])

# --------------------- Run ---------------------
if st.button("ğŸš€ åŸ·è¡Œ"):
    if task in ["Question Answering", "Summarization", "Named Entity Recognition", "Zero-shot Classification"] and not source_text.strip():
        st.warning("è«‹å…ˆæä¾›æ–‡ç« /å…§å®¹ä¾†æºï¼ˆä¸Šå‚³ / è²¼ä¸Š / æŠ“å–ç¶²å€ï¼‰ã€‚")
        st.stop()

    if task == "Sentiment Analysis" and not (source_text.strip() or pasted.strip() or uploaded or st.session_state.get("fetched_text")):
        st.warning("è«‹æä¾›æ–‡å­—ï¼ˆå¯ä¸Šå‚³æª”æ¡ˆã€è²¼ä¸Šæˆ–æŠ“å–ç¶²å€ï¼‰ã€‚")
        st.stop()

    # Build pipeline
    if task == "Sentiment Analysis":
        clf = get_pipeline("sentiment-analysis", model_id, tokenizer_id)
        units = [u.strip() for u in source_text.splitlines() if u.strip()]
        if not units:
            st.warning("æœªå–å¾—å¯åˆ†æçš„å¥å­ï¼Œè«‹ç¢ºèªå…§å®¹ã€‚")
            st.stop()
        with st.spinner("åˆ†æä¸­â€¦"):
            results = clf(units, truncation=True)
        st.subheader("çµæœ")
        for u, r in zip(units, results):
            st.write(f"- **{r['label']}** ({r.get('score', 0):.3f}) â€” {u}")
        out = [{"text": u, **r} for u, r in zip(units, results)]
        show_json_download(out, "ä¸‹è¼‰çµæœ JSON", "sentiment_results.json")

    elif task == "Question Answering":
        qa = get_pipeline("question-answering", model_id, tokenizer_id)
        q = question.strip() if 'question' in locals() else ""
        if not q:
            st.warning("è«‹è¼¸å…¥å•é¡Œæ–‡å­—ã€‚")
            st.stop()
        with st.spinner("å›ç­”ä¸­â€¦"):
            ans = qa(question=q, context=source_text, handle_impossible_answer=True, max_answer_len=64)
        st.subheader("ç­”æ¡ˆ")
        st.markdown(f"**Answer:** {ans.get('answer', '')}  \\n**Score:** {ans.get('score', 0):.3f}  \\n**Start-End:** {ans.get('start')} - {ans.get('end')}")
        show_json_download(ans, "ä¸‹è¼‰çµæœ JSON", "qa_result.json")

    elif task == "Summarization":
        summarizer = get_pipeline("summarization", model_id, tokenizer_id)
        chunks = chunk_text(source_text, max_chars=1500, overlap=50)
        if not chunks:
            st.warning("ç„¡æ³•åˆ‡åˆ†å…§å®¹ï¼Œè«‹ç¢ºèªæ–‡å­—ä¾†æºã€‚")
            st.stop()
        with st.spinner(f"æ‘˜è¦ä¸­ï¼ˆå…± {len(chunks)} å¡Šï¼‰â€¦"):
            partial = []
            for i, ch in enumerate(chunks, 1):
                res = summarizer(ch, max_length=min(512, max_new_tokens or 256), min_length=16, do_sample=False)
                partial.append(res[0]["summary_text"])
        final_summary = " ".join(partial)
        st.subheader("æ‘˜è¦")
        st.write(final_summary)
        show_json_download({"summary": final_summary, "chunks": partial}, "ä¸‹è¼‰çµæœ JSON", "summary.json")

    elif task == "Named Entity Recognition":
        ner_pipe = get_pipeline("ner", model_id, tokenizer_id)
        with st.spinner("è¾¨è­˜ä¸­â€¦"):
            ents = ner_pipe(source_text, grouped_entities=True, aggregation_strategy="simple")
        st.subheader("å‘½åå¯¦é«”")
        if not ents:
            st.info("æ²’æœ‰åµæ¸¬åˆ°å¯¦é«”ã€‚")
        else:
            for e in ents:
                label = e.get("entity_group") or e.get("entity", "")
                score = e.get("score", 0.0)
                word = e.get("word", "")
                st.write(f"- **{label}** ({score:.3f}): {word}")
        show_json_download(ents, "ä¸‹è¼‰çµæœ JSON", "ner.json")

    elif task == "Zero-shot Classification":
        zsc = get_pipeline("zero-shot-classification", model_id, tokenizer_id)
        candidate_labels = [s.strip() for s in labels_text.split(",") if s.strip()] if 'labels_text' in locals() else []
        if not candidate_labels:
            st.warning("è«‹æä¾›è‡³å°‘ä¸€å€‹æ¨™ç±¤ï¼ˆä»¥é€—è™Ÿåˆ†éš”ï¼‰ã€‚")
            st.stop()
        with st.spinner("åˆ†é¡ä¸­â€¦"):
            res = zsc(source_text, candidate_labels=candidate_labels, multi_label=bool(multi_label))
        st.subheader("åˆ†é¡çµæœ")
        if isinstance(res, dict) and "labels" in res:
            st.write("**labels** :", res["labels"])
            st.write("**scores** :", [round(x, 3) for x in res["scores"]])
            show_json_download(res, "ä¸‹è¼‰çµæœ JSON", "zero_shot.json")
        else:
            st.write(res)
            show_json_download(res, "ä¸‹è¼‰çµæœ JSON", "zero_shot.json")

st.markdown("---")
st.caption("Tips: è‹¥è¦ä½¿ç”¨ GPUï¼Œè«‹å®‰è£å°æ‡‰çš„ PyTorch CUDA ç‰ˆæœ¬ï¼›å¦‚é‡åˆ°ç¶²å€é™åˆ¶ï¼ˆéœ€è¦ç™»å…¥/é˜²çˆ¬ï¼‰ï¼Œå¯å…ˆæ‰‹å‹•è¤‡è£½å…§æ–‡è²¼ä¸Šä½¿ç”¨ã€‚")
